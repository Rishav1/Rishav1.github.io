---
layout: post
title:  "Using Generative Models for emulating RL Systems"
date:   2017-10-14
desc: "Using Generative Models for emulating RL Systems"
keywords: "Reinforcement Learning, Generative Models, Discriminative Models, DQN"
categories: [Rein_learning]
tags: [Reinforcement Learning, Generative Models, Discriminative Models, DQN]
icon: icon-html
---

The widely accepted decoupling used in Reinforcement Learning is to treat controllable entity as agent and rest of the system as the  environment. The agent receives observations about the environment and takes reward accruing action that affects the environment. Access to environmental interaction bestows upon RL agents the capacity to keep on learning and adapting as the environment adapts. This benefit of RL agents comes with the heavy cost that unlimited interaction with environments are needed.  
{: .text-justify}

<p align="center"><img src="/static/assets/img/blog/GAN/RL_agent_env.png" alt="blog-image" width="500" height="300"></p>
<p align="center">An example MDP with three states corresponding actions with transition probabilities.</p>

In many cases we either don't have unlimited access, like in online games where per day playtime is limited, or have an unlimited access but at very slow observation generation, like in case of stock market where current stock prices are updated per second(very slow in computer's perspective). This begs a natural question, can we simulate the environment simultaneously then? If possible, it will definitely reduce costs associated with actual environment. The answer is yes, it can be done, and is actually being done in many areas. Many real-life problems have already been converted into interactive simulations. Checkout environments created by [RoboSchool project](https://blog.openai.com/roboschool/) and [MuJoCo project](http://www.mujoco.org/index.html).
{: .text-justify}

<div align="center">
<video autoplay="True"  width="640" height="480"  controls loop preload='metadata' onclick='(function(el){ if(el.paused) el.play(); else el.pause() })(this)'>
  <source src='https://storage.googleapis.com/joschu-public/demo-race.mp4' type='video/mp4; codecs="avc1.42E01E, mp4a.40.2"'>
</video>
<p>RoboSchool project environment example.</p>
</div>

These are indeed great strides on simulating real-life components. But in general, creating a simulator for an environment is a tremendously tough task, and most of the times use idealistic models that are measurably different from real life. Can something be done to ameliorate it? How about we use generative models to partially represent environments? Feynman once said, "What I cannot create, I don't understand." Surely if we want to create a simulator for an environment, we must have comprehensive knowledge about how the environment works. Environments are actually more complex than what generative models can emulate. An interactive environment has an associated time dependent dynamicity as well as a dependence on states and actions. In other words, for each state-action pair **(s,a)**, environment specifies a probability distribution for the next state **s'** and the associated reward **r**. In a time dynamic environment, this probability distribution also changes with time. 
{: .text-justify}

<p align="center"><img src="/static/assets/img/blog/GAN/GAN_example.png" alt="blog-image" width="500" height="300"></p>
<p align="center"><img src="/static/assets/img/blog/GAN/GAN_transformation.png" alt="blog-image" width="500" height="300"></p>
<p align="center">An trained GAN mapping uniform random distribution to distribution of images of faces.</p>

Generative models on the other hand assumes a static probability distribution for the data. Moreover their training process have been computationally more costly due to the objective-function requiring multiple forward and backward passes. Also the entropy in the unconditional probability **P(x)** in case of generative model is much bigger than the entropy in the conditional probability **P(y\|x)** in case of discriminative model. A crude implication is that training a generative neural network would be much more difficult than training a discriminative neural network.
{: .text-justify}

Ignoring all these shortcomings, and assuming that in future we could come up with a mechanism for efficiently training a generative model so fast that even time-dynamicity can be accounted for, how would one go about using this model for approximating an environment? The traditional RL algorithm Q-Learning is based on markov chains of sequences **s -> a -> r -> s' -> a' ...** which cannot be sequentially generated by the generative models. What these models however can generate is tuples of the form **(s, a, r, s')** representing various instances where on some state **s** the agent took an action **a** getting a reward **r** and next state **s'**. If you observe, the probability distribution of these tuples not only contains information about the environment, but also about the action choices of the agent. **The generative model will actually capture the entire agent-environment system.** Note however that these tuples are disconnected and don't form a markov chain. Surely the Q-learning, without modification cannot use the models, but it is possible to modify Q-learning subtly enough to achieve optimal policy convergence using these models. As a matter of fact, Q-Learning based Google's Deep Q-Learning Network can also be similarly modified to use generative models. 
{: .text-justify}

<p align="center"><img src="/static/assets/img/blog/GAN/qalg.gif" alt="blog-image" width="600" height="300"></p>
<p align="center">Q-Learning algorithm.</p>

First lets decide some notations. Let **x** be a random variable sampled from uniform distribution, and the generative model **G(x)** be a mapping from x to a tuple **(s, a, r, s')**. In essence, the trained model **G** represents the system's properties in form of probability distribution of the tuples. The Q-Learning algorithm is shown in the image above. It is worth noting that if generative models are used for representing system, the concept of episodes cease to exist, i.e. their is no Markov chain to follow. 
{: .text-justify}

Since the update equation **Q(s,a) <-- Q(s,a) + α[r + γ * max[Q(s',a') - Q(s,a)]]** can be used for tuples **(s, a, r, s')** generated from a trained model **G**, we can use **G** to find out the current policy **Q**-table by repeatedly sampling tuples from **G** and using the update equation to better accurate **Q**. Also, we assumed that generative model can be easily trained implying that we can easily obtain updated model **G'** that now accounts for system's behavior on updated agent's policy. 
{: .text-justify}

In summary, the algorithm would be a chain of generative model updates and Policy updates of the form **Q<sub>i</sub> -> G<sub>i</sub> -> Q<sub>i+1</sub> -> G<sub>i+1</sub>** that would go on until both our Generative model and policy table converges. The benefit of this lies in the fact that while training policy, a model of the system, i.e. **G** is being used instead of the actual environment and thus the interaction restrictions implicit to the environment can be done away to an extent. It is obvious however that for training the model **G** substantial interaction with the environment will be required.

**The power of this technique comes to light in cases such as stock markets or micro trading where a successful trader's history of decisions can be leveraged to train an initial generative model *G<sub>0</sub>* and the above method can be used to get extract ever improving policies *Q<sub>i</sub>* strictly better than the trader's policy *Q<sub>0</sub>*.**
